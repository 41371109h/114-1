{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371109h/114-1/blob/main/hw4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£æ‰€æœ‰å¿…è¦çš„ Python å¥—ä»¶\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "# ç¢ºä¿ jieba å­—å…¸æº–å‚™å°±ç·’ (éå¿…è¦ï¼Œä½†æœ‰åŠ©æ–¼ç¢ºä¿ç¬¬ä¸€æ¬¡é‹è¡ŒæˆåŠŸ)\n",
        "import jieba\n",
        "try:\n",
        "    jieba.lcut(\"åˆå§‹åŒ–\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "AwdmI3LcClBt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from google.colab import auth, userdata\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth import default\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import tee\n",
        "\n",
        "# --- Google Colab èªè­‰ (å…è¨±å­˜å– Sheets) ---\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# --- Gemini API è¨­å®š ---\n",
        "try:\n",
        "    # å¾ Colab Secrets ä¸­ç²å– API é‡‘é‘°\n",
        "    api_key = userdata.get('wt')\n",
        "    genai.configure(api_key=api_key)\n",
        "    # ä½¿ç”¨ 2.5 Flash é€²è¡Œå¿«é€Ÿæ´å¯Ÿç”Ÿæˆ\n",
        "    GEMINI_MODEL = genai.GenerativeModel('gemini-2.5-flash')\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Gemini API Key è¨­ç½®å¤±æ•—æˆ–ç„¡æ³•è®€å–ï¼š{e}\")\n",
        "    GEMINI_MODEL = None\n",
        "\n",
        "\n",
        "# --- å¸¸æ•¸å®šç¾© ---\n",
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1eqVMllof4j4Xxvmnzny7z8ojKifHj-CS3a1IUJ2brfg/edit?gid=0#gid=0\"\n",
        "WORKSHEET_NAME = \"PTTé›»å½±åˆ†æ\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]\n",
        "\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n"
      ],
      "metadata": {
        "id": "Nj8iUKM0ClEB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Sheet è¼”åŠ©å‡½å¼ ---\n",
        "def ensure_spreadsheet(name):\n",
        "    \"\"\"ç¢ºä¿è©¦ç®—è¡¨å­˜åœ¨ï¼Œè‹¥ç„¡å‰‡å‰µå»º\"\"\"\n",
        "    try:\n",
        "        sh = gc.open(name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "def ensure_worksheet(sh, title, header):\n",
        "    \"\"\"ç¢ºä¿å·¥ä½œè¡¨å­˜åœ¨ï¼Œä¸¦æª¢æŸ¥/è¨­å®šæ­£ç¢ºçš„è¡¨é ­\"\"\"\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # æª¢æŸ¥ä¸¦ä¿®æ­£è¡¨é ­\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws\n",
        "\n",
        "def tznow():\n",
        "    \"\"\"å›å‚³å¸¶æœ‰æ™‚å€çš„æ™‚é–“æˆ³è¨˜\"\"\"\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "def read_df(ws, header):\n",
        "    \"\"\"å¾ Worksheet è®€å–è³‡æ–™ç‚º DataFrameï¼Œä¸¦è™•ç†ç©ºå€¼èˆ‡å‹åˆ¥\"\"\"\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ•¸å­—ï¼ˆæ­¤è™•æˆ‘å€‘åªä¿ç•™å­—ä¸²å‹æ…‹ï¼Œè®“ pandas è™•ç†ï¼‰\n",
        "    return df[header]\n",
        "\n",
        "def write_df(ws, df, header):\n",
        "    \"\"\"å°‡ DataFrame å¯«å…¥ Worksheet\"\"\"\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "    # è½‰å­—ä¸²é¿å… gspread å‹åˆ¥å•é¡Œ\n",
        "    df_out = df.copy()\n",
        "    for c in df_out.columns:\n",
        "        df_out[c] = df_out[c].astype(str)\n",
        "    ws.clear()\n",
        "    ws.update([header] + df_out.values.tolist())\n",
        "\n",
        "def read_ptt_posts_df():\n",
        "    \"\"\"è®€å– PTT è²¼æ–‡è³‡æ–™æ¡†\"\"\"\n",
        "    return read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "\n",
        "def read_terms_df():\n",
        "    \"\"\"è®€å–é—œéµè©è³‡æ–™æ¡†\"\"\"\n",
        "    return read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "\n",
        "\n",
        "# --- åˆå§‹åŒ–å·¥ä½œè¡¨èˆ‡ DataFrame ---\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)\n",
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)\n",
        "\n",
        "# å…¨åŸŸ DataFrame è®Šæ•¸åˆå§‹åŒ– (ç”¨æ–¼ç¨‹å¼å…§å­˜æ“ä½œ)\n",
        "ptt_posts_df = read_ptt_posts_df()\n",
        "terms_df = read_terms_df()"
      ],
      "metadata": {
        "id": "gTsDOW_cClGY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PTT é›»å½±ç‰ˆçˆ¬èŸ²å‡½å¼ç¾¤ ---\n",
        "def _get_soup(url):\n",
        "    \"\"\"ç™¼é€è«‹æ±‚ä¸¦ç²å– BeautifulSoup ç‰©ä»¶\"\"\"\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    \"\"\"ç²å–ä¸Šä¸€é çš„ URL\"\"\"\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    for a in btns:\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    \"\"\"è§£ææ¨æ–‡æ•¸ (çˆ†/X/æ•¸å­—)\"\"\"\n",
        "    if not nrec_span: return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\": return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    \"\"\"å¾ç´¢å¼•é é¢æå–æ–‡ç« åˆ—è¡¨\"\"\"\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a: continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    \"\"\"æ¸…é™¤å…§æ–‡ä¸­çš„æ¨æ–‡å€å’Œ meta è³‡è¨Šï¼Œæå–ä¹¾æ·¨çš„å…§æ–‡\"\"\"\n",
        "    for p in soup.select(\"div.push\"): p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main: return \"\", \"\"\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas: m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"å¾æœ€æ–° index.html å¾€å‰ç¿»é æŠ“å–æ–‡ç« ï¼Œä¸¦å¯«å…¥ Google Sheet\"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    # å¾ç•¶å‰ DataFrame ç²å–å·²æœ‰çš„ URL é€²è¡Œå»é‡\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for page_num in range(int(index_pages)):\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "            posts = _extract_post_list(soup)\n",
        "        except Exception as e:\n",
        "            print(f\"çˆ¬å–ç¬¬ {page_num+1} é å¤±æ•—ï¼š{e}\")\n",
        "            break\n",
        "\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push): continue\n",
        "            if keyword and (keyword not in p[\"title\"]): continue\n",
        "            if p[\"url\"] in seen_urls: continue\n",
        "\n",
        "            # æŠ“æ­£æ–‡\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception:\n",
        "                content, meta_title = \"\", \"\"\n",
        "\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"ï¼ˆç„¡æ¨™é¡Œï¼‰\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev: break\n",
        "        url = prev\n",
        "        time.sleep(0.5) # é¿å…è¢«é–\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        # å¯«å› Google Sheet\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        total_count = len(ptt_posts_df)\n",
        "        return f\"âœ… æ–°å¢ {len(all_rows)} ç¯‡æ–‡ç« è‡³ Sheetï¼Œç›®å‰ç¸½æ•¸ï¼š{total_count} ç¯‡\", ptt_posts_df\n",
        "    else:\n",
        "        total_count = len(ptt_posts_df)\n",
        "        return f\"â„¹ï¸ æ²’æœ‰æ–°æ–‡ç« ç¬¦åˆæ¢ä»¶æˆ–å…§å®¹å·²å­˜åœ¨ã€‚ç›®å‰ç¸½æ•¸ï¼š{total_count} ç¯‡\", ptt_posts_df\n",
        "\n",
        "# --- æ–‡æœ¬åˆ†æèˆ‡é—œéµè©çµ±è¨ˆ ---\n",
        "def _tokenize_zh(text):\n",
        "    \"\"\"Jieba åˆ†è©ï¼Œä¸¦éæ¿¾æ‰é•·åº¦å°æ–¼ 2 çš„è©å½™\"\"\"\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        return [t for t in text.split() if len(t) > 1]\n",
        "    return [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "def analyze_ptt_texts(topk=50, min_df=2):\n",
        "    \"\"\"åŸ·è¡Œ TF-IDF é—œéµè©åˆ†æï¼Œä¸¦å°‡çµæœå¯«å…¥ Google Sheet\"\"\"\n",
        "    global ptt_posts_df, terms_df\n",
        "    if ptt_posts_df.empty:\n",
        "        terms_df = pd.DataFrame(columns=TERMS_HEADER)\n",
        "        return \"ğŸ“­ å°šç„¡å·²æŠ“å–çš„æ–‡ç« ï¼Œè«‹å…ˆåŸ·è¡Œçˆ¬èŸ²ã€‚\", terms_df, \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        # å°‡æ¨™é¡Œèˆ‡å…§æ–‡æ‹¼èµ·ä¾†ä½œç‚ºæ–‡ä»¶\n",
        "        docs.append((r[\"title\"] or \"\") + \" \" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # --- è©é »çµ±è¨ˆ ---\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # --- TF-IDF è¨ˆç®— ---\n",
        "    try:\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=int(min_df))\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception:\n",
        "        tfidf_map = {t: 0.0 for t in freq.keys()}\n",
        "\n",
        "    # --- æ’åºèˆ‡ TopK é—œéµè©æå– ---\n",
        "    candidates = [t for t in freq.keys() if t in tfidf_map]\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0), 6), freq[t]), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    # --- ç¯„ä¾‹å¥æå– ---\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    # --- è¼¸å‡º DataFrame ---\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "\n",
        "    # å¯«å› Google Sheet\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "    msg_status = f\"âœ… å·²å®Œæˆæ–‡æœ¬åˆ†æï¼Œå…± {len(docs)} ç¯‡æ–‡ç« ï¼›å·²å›å¯« Top {len(top_terms)} é—œéµè©è‡³ Sheetã€‚\"\n",
        "\n",
        "    # ç”¢ç”Ÿ Markdown æ‘˜è¦çµ¦å ±å‘Šç”¨\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### é—œéµè© Top {len(top_terms)}ï¼ˆä¾ TF-IDF å¹³å‡å€¼å„ªå…ˆï¼Œæ¬¡åºå†ä»¥è©é »ï¼‰\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** â€” tfidfâ‰ˆ{float(tfidf_map.get(t,0.0)):.4f}ï¼›freq={freq[t]}ï¼›df={df_cnt[t]}\")\n",
        "\n",
        "    return msg_status, terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "id": "H06pA8aBClIc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_insights(df_terms, model):\n",
        "    \"\"\"\n",
        "    ä¸²æ¥ Gemini APIï¼Œæ ¹æ“š Top é—œéµè©ç”Ÿæˆ 5 å¥æ´å¯Ÿæ‘˜è¦å’Œ 120 å­—çµè«–ã€‚\n",
        "    \"\"\"\n",
        "    if model is None or df_terms.empty:\n",
        "        return \"âš ï¸ Gemini API æœªåˆå§‹åŒ–æˆ–é—œéµè©åˆ†æçµæœç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆ AI æ´å¯Ÿã€‚\"\n",
        "\n",
        "    # é¸æ“‡å‰ 10 å€‹è©å½™åŠå…¶ TF-IDF å€¼\n",
        "    top_terms_data = df_terms.head(10)[[\"term\", \"tfidf_mean\", \"freq\", \"examples\"]].to_dict('records')\n",
        "\n",
        "    # æ§‹å»ºçµ¦ Gemini çš„æŒ‡ä»¤\n",
        "    system_prompt = (\n",
        "        \"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é›»å½±å¸‚å ´èˆ‡è¼¿æƒ…åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šè¼¸å…¥çš„ PTT é—œéµè©èˆ‡çµ±è¨ˆæ•¸æ“šï¼Œ\"\n",
        "        \"ç¸½çµå‡ºå°ç•¶å‰é›»å½±è©±é¡Œçš„æ´å¯Ÿã€‚è«‹å‹™å¿…åš´æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼è¦æ±‚ï¼š\"\n",
        "        \"1. **æ´å¯Ÿæ‘˜è¦**: è¼¸å‡º 5 å€‹ç²¾ç¢ºçš„å¥å­ï¼Œæ¯å€‹å¥å­éƒ½å¿…é ˆä»¥ç·¨è™Ÿ 1. åˆ° 5. é–‹é ­ã€‚\"\n",
        "        \"2. **çµè«–**: è¼¸å‡ºä¸€æ®µç¸½é•·åº¦ç´„ 120 å­—çš„ç¸½çµæ®µè½ã€‚è©²çµè«–å¿…é ˆåœ¨æ´å¯Ÿæ‘˜è¦ä¹‹å¾Œï¼Œä¸¦ä»¥æ¨™é¡Œ '## åˆ†æçµè«–' é–‹é ­ã€‚\"\n",
        "        \"è«‹åªå›å‚³è«‹æ±‚çš„ Markdown å…§å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„å‰è¨€æˆ–å¾Œè¨˜ã€‚\"\n",
        "    )\n",
        "\n",
        "    user_query = f\"è«‹æ ¹æ“šä»¥ä¸‹ PTT é›»å½±ç‰ˆç†±é–€é—œéµè©ï¼ˆå·²æŒ‰ TF-IDF å¹³å‡å€¼æ’åºï¼‰é€²è¡Œåˆ†æï¼š\\n\\n{json.dumps(top_terms_data, ensure_ascii=False)}\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            contents=[\n",
        "                {\"role\": \"user\", \"parts\": [{\"text\": user_query}]}\n",
        "            ],\n",
        "            system_instruction=system_prompt,\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Gemini API å‘¼å«å¤±æ•—ï¼š{e}\"\n",
        "\n",
        "\n",
        "def full_analysis_workflow(index_pages, min_push, keyword, topk, min_df):\n",
        "    \"\"\"\n",
        "    ä¸€éµåŸ·è¡Œå®Œæ•´çš„çˆ¬èŸ²ã€åˆ†æå’Œ AI æ´å¯Ÿæµç¨‹ã€‚\n",
        "    \"\"\"\n",
        "    # 1. çˆ¬èŸ²éšæ®µ\n",
        "    msg_crawl, df_posts = crawl_ptt_movie(index_pages, min_push, keyword)\n",
        "\n",
        "    # 2. æ–‡æœ¬åˆ†æéšæ®µ (å¾ Sheet è®€å›æœ€æ–°æ•¸æ“š)\n",
        "    global ptt_posts_df\n",
        "    ptt_posts_df = df_posts # ä½¿ç”¨å‰›çˆ¬å–/åˆä½µçš„æœ€æ–° DF é€²è¡Œåˆ†æ\n",
        "    msg_analysis, df_terms, md_analysis = analyze_ptt_texts(topk=topk, min_df=min_df)\n",
        "\n",
        "    # 3. AI æ´å¯Ÿç”Ÿæˆéšæ®µ\n",
        "    gemini_output = generate_insights(df_terms, GEMINI_MODEL)\n",
        "\n",
        "    # 4. çµ„åˆæ‰€æœ‰çµæœ\n",
        "    final_report = f\"\"\"\n",
        "# PTT é›»å½±ç‰ˆè¼¿æƒ…åˆ†æå ±å‘Š\n",
        "\n",
        "## æµç¨‹ç‹€æ…‹\n",
        "- çˆ¬èŸ²çµæœï¼š{msg_crawl}\n",
        "- æ–‡æœ¬åˆ†æï¼š{msg_analysis}\n",
        "\n",
        "---\n",
        "\n",
        "## é—œéµè©èˆ‡è©é »çµ±è¨ˆ\n",
        "{md_analysis}\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¤– Gemini AI æ´å¯Ÿèˆ‡ç¸½çµ\n",
        "\n",
        "{gemini_output}\n",
        "\"\"\"\n",
        "    return final_report, df_terms\n",
        "\n",
        "def _refresh_data():\n",
        "    \"\"\"é‡æ–°å¾ Sheet è®€å–æœ€æ–°æ•¸æ“šåˆ°å…§å­˜\"\"\"\n",
        "    global ptt_posts_df, terms_df\n",
        "    ptt_posts_df = read_ptt_posts_df()\n",
        "    terms_df = read_terms_df()\n",
        "    return ptt_posts_df, terms_df\n"
      ],
      "metadata": {
        "id": "c2-aQ7vzClKh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥æœ€æ–°çš„æ•¸æ“šä»¥ä¾›åˆå§‹åŒ–é¡¯ç¤º\n",
        "_refresh_data()\n",
        "\n",
        "with gr.Blocks(title=\"PTT é›»å½±ç‰ˆ AI è¼¿æƒ…åˆ†æ\") as demo:\n",
        "    gr.Markdown(\"# ğŸ¬ PTT é›»å½±ç‰ˆ AI è¼¿æƒ…åˆ†æè‡ªå‹•åŒ–å·¥å…·\")\n",
        "    gr.Markdown(\"æ­¤å·¥å…·æœƒåŸ·è¡Œï¼š**PTT çˆ¬èŸ² â†’ å¯«å…¥ Sheet â†’ TF-IDF é—œéµè©åˆ†æ â†’ å¯«å…¥çµ±è¨ˆè¡¨ â†’ Gemini AI æ´å¯Ÿç”Ÿæˆ**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(f\"**Google Sheet URL:** [åˆ†æè©¦ç®—è¡¨é€£çµ]({SHEET_URL})\")\n",
        "\n",
        "    with gr.Tab(\"ä¸€éµåˆ†æåŸ·è¡Œå€\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### çˆ¬èŸ²åƒæ•¸ (Crawl Settings)\")\n",
        "                index_pages = gr.Number(label=\"å¾€å‰ç¿»é æ•¸ (Pages)\", value=3, precision=0)\n",
        "                min_push = gr.Number(label=\"æœ€å°æ¨æ–‡æ•¸ (Min Nrec)\", value=10, precision=0)\n",
        "                keyword = gr.Textbox(label=\"æ¨™é¡Œé—œéµè©ç¯©é¸ (Keyword)\", value=\"\", placeholder=\"å¯é¸ï¼Œä¾‹å¦‚ï¼šé˜¿å‡¡é”\")\n",
        "\n",
        "                gr.Markdown(\"### æ–‡æœ¬åˆ†æåƒæ•¸ (Analysis Settings)\")\n",
        "                topk = gr.Number(label=\"è¼¸å‡º Top K é—œéµè©\", value=50, precision=0)\n",
        "                min_df = gr.Number(label=\"æœ€å°æ–‡ä»¶é »ç‡ (Min DF)\", value=2, precision=0, info=\"è©å½™è‡³å°‘è¦åœ¨å¹¾ç¯‡æ–‡ç« ä¸­å‡ºç¾\")\n",
        "\n",
        "                btn_run = gr.Button(\"ğŸš€ ä¸€éµåŸ·è¡Œå®Œæ•´æµç¨‹ä¸¦ç”Ÿæˆå ±å‘Š\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### ğŸ¤– åˆ†æå ±å‘Š (Analysis Report)\")\n",
        "                report_output = gr.Markdown(\"é»æ“ŠæŒ‰éˆ•é–‹å§‹åˆ†æ...\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### ğŸ’¾ é—œéµè©çµ±è¨ˆçµæœé è¦½ (Top Terms Preview)\")\n",
        "        grid_terms = gr.Dataframe(value=terms_df, label=\"é—œéµè©çµ±è¨ˆæ•¸æ“š (å·²å¯«å…¥ Sheet)\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"åŸå§‹è²¼æ–‡é è¦½\"):\n",
        "        gr.Markdown(\"### åŸå§‹è²¼æ–‡æ•¸æ“š (å·²ç´¯ç©åœ¨ Sheet ä¸­)\")\n",
        "        grid_posts = gr.Dataframe(value=ptt_posts_df, label=\"PTT æ–‡ç« è³‡æ–™\", interactive=False)\n",
        "        btn_refresh_data = gr.Button(\"â¬‡ï¸ å¾ Sheet è®€å–æœ€æ–°æ•¸æ“š\")\n",
        "\n",
        "    # --- ç¶å®šå‹•ä½œ ---\n",
        "\n",
        "    # åŸ·è¡Œæµç¨‹\n",
        "    btn_run.click(\n",
        "        full_analysis_workflow,\n",
        "        inputs=[index_pages, min_push, keyword, topk, min_df],\n",
        "        outputs=[report_output, grid_terms]\n",
        "    )\n",
        "\n",
        "    # é‡æ–°è®€å–æ•¸æ“š\n",
        "    btn_refresh_data.click(\n",
        "        _refresh_data,\n",
        "        outputs=[grid_posts, grid_terms]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "b2hVyzl7ClMn",
        "outputId": "d4c68d48-75c2-4a77-83e4-10dcbd40df10"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://247a988d55b4f26592.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://247a988d55b4f26592.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}