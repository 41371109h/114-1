{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371109h/114-1/blob/main/hw4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝所有必要的 Python 套件\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "# 確保 jieba 字典準備就緒 (非必要，但有助於確保第一次運行成功)\n",
        "import jieba\n",
        "try:\n",
        "    jieba.lcut(\"初始化\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "AwdmI3LcClBt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from google.colab import auth, userdata\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth import default\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import tee\n",
        "\n",
        "# --- Google Colab 認證 (允許存取 Sheets) ---\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# --- Gemini API 設定 ---\n",
        "try:\n",
        "    # 從 Colab Secrets 中獲取 API 金鑰\n",
        "    api_key = userdata.get('wt')\n",
        "    genai.configure(api_key=api_key)\n",
        "    # 使用 2.5 Flash 進行快速洞察生成\n",
        "    GEMINI_MODEL = genai.GenerativeModel('gemini-2.5-flash')\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Gemini API Key 設置失敗或無法讀取：{e}\")\n",
        "    GEMINI_MODEL = None\n",
        "\n",
        "\n",
        "# --- 常數定義 ---\n",
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1eqVMllof4j4Xxvmnzny7z8ojKifHj-CS3a1IUJ2brfg/edit?gid=0#gid=0\"\n",
        "WORKSHEET_NAME = \"PTT電影分析\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]\n",
        "\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n"
      ],
      "metadata": {
        "id": "Nj8iUKM0ClEB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Sheet 輔助函式 ---\n",
        "def ensure_spreadsheet(name):\n",
        "    \"\"\"確保試算表存在，若無則創建\"\"\"\n",
        "    try:\n",
        "        sh = gc.open(name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "def ensure_worksheet(sh, title, header):\n",
        "    \"\"\"確保工作表存在，並檢查/設定正確的表頭\"\"\"\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # 檢查並修正表頭\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws\n",
        "\n",
        "def tznow():\n",
        "    \"\"\"回傳帶有時區的時間戳記\"\"\"\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "def read_df(ws, header):\n",
        "    \"\"\"從 Worksheet 讀取資料為 DataFrame，並處理空值與型別\"\"\"\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # 確保數值欄位為數字（此處我們只保留字串型態，讓 pandas 處理）\n",
        "    return df[header]\n",
        "\n",
        "def write_df(ws, df, header):\n",
        "    \"\"\"將 DataFrame 寫入 Worksheet\"\"\"\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "    # 轉字串避免 gspread 型別問題\n",
        "    df_out = df.copy()\n",
        "    for c in df_out.columns:\n",
        "        df_out[c] = df_out[c].astype(str)\n",
        "    ws.clear()\n",
        "    ws.update([header] + df_out.values.tolist())\n",
        "\n",
        "def read_ptt_posts_df():\n",
        "    \"\"\"讀取 PTT 貼文資料框\"\"\"\n",
        "    return read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "\n",
        "def read_terms_df():\n",
        "    \"\"\"讀取關鍵詞資料框\"\"\"\n",
        "    return read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "\n",
        "\n",
        "# --- 初始化工作表與 DataFrame ---\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)\n",
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)\n",
        "\n",
        "# 全域 DataFrame 變數初始化 (用於程式內存操作)\n",
        "ptt_posts_df = read_ptt_posts_df()\n",
        "terms_df = read_terms_df()"
      ],
      "metadata": {
        "id": "gTsDOW_cClGY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PTT 電影版爬蟲函式群 ---\n",
        "def _get_soup(url):\n",
        "    \"\"\"發送請求並獲取 BeautifulSoup 物件\"\"\"\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    \"\"\"獲取上一頁的 URL\"\"\"\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    for a in btns:\n",
        "        if \"上頁\" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    \"\"\"解析推文數 (爆/X/數字)\"\"\"\n",
        "    if not nrec_span: return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"爆\": return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    \"\"\"從索引頁面提取文章列表\"\"\"\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a: continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    \"\"\"清除內文中的推文區和 meta 資訊，提取乾淨的內文\"\"\"\n",
        "    for p in soup.select(\"div.push\"): p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main: return \"\", \"\"\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas: m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"從最新 index.html 往前翻頁抓取文章，並寫入 Google Sheet\"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    # 從當前 DataFrame 獲取已有的 URL 進行去重\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for page_num in range(int(index_pages)):\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "            posts = _extract_post_list(soup)\n",
        "        except Exception as e:\n",
        "            print(f\"爬取第 {page_num+1} 頁失敗：{e}\")\n",
        "            break\n",
        "\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push): continue\n",
        "            if keyword and (keyword not in p[\"title\"]): continue\n",
        "            if p[\"url\"] in seen_urls: continue\n",
        "\n",
        "            # 抓正文\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception:\n",
        "                content, meta_title = \"\", \"\"\n",
        "\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"（無標題）\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev: break\n",
        "        url = prev\n",
        "        time.sleep(0.5) # 避免被鎖\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        # 寫回 Google Sheet\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        total_count = len(ptt_posts_df)\n",
        "        return f\"✅ 新增 {len(all_rows)} 篇文章至 Sheet，目前總數：{total_count} 篇\", ptt_posts_df\n",
        "    else:\n",
        "        total_count = len(ptt_posts_df)\n",
        "        return f\"ℹ️ 沒有新文章符合條件或內容已存在。目前總數：{total_count} 篇\", ptt_posts_df\n",
        "\n",
        "# --- 文本分析與關鍵詞統計 ---\n",
        "def _tokenize_zh(text):\n",
        "    \"\"\"Jieba 分詞，並過濾掉長度小於 2 的詞彙\"\"\"\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        return [t for t in text.split() if len(t) > 1]\n",
        "    return [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "def analyze_ptt_texts(topk=50, min_df=2):\n",
        "    \"\"\"執行 TF-IDF 關鍵詞分析，並將結果寫入 Google Sheet\"\"\"\n",
        "    global ptt_posts_df, terms_df\n",
        "    if ptt_posts_df.empty:\n",
        "        terms_df = pd.DataFrame(columns=TERMS_HEADER)\n",
        "        return \"📭 尚無已抓取的文章，請先執行爬蟲。\", terms_df, \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        # 將標題與內文拼起來作為文件\n",
        "        docs.append((r[\"title\"] or \"\") + \" \" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # --- 詞頻統計 ---\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # --- TF-IDF 計算 ---\n",
        "    try:\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=int(min_df))\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception:\n",
        "        tfidf_map = {t: 0.0 for t in freq.keys()}\n",
        "\n",
        "    # --- 排序與 TopK 關鍵詞提取 ---\n",
        "    candidates = [t for t in freq.keys() if t in tfidf_map]\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0), 6), freq[t]), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    # --- 範例句提取 ---\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    # --- 輸出 DataFrame ---\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "\n",
        "    # 寫回 Google Sheet\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "    msg_status = f\"✅ 已完成文本分析，共 {len(docs)} 篇文章；已回寫 Top {len(top_terms)} 關鍵詞至 Sheet。\"\n",
        "\n",
        "    # 產生 Markdown 摘要給報告用\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### 關鍵詞 Top {len(top_terms)}（依 TF-IDF 平均值優先，次序再以詞頻）\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** — tfidf≈{float(tfidf_map.get(t,0.0)):.4f}；freq={freq[t]}；df={df_cnt[t]}\")\n",
        "\n",
        "    return msg_status, terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "id": "H06pA8aBClIc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_insights(df_terms, model):\n",
        "    \"\"\"\n",
        "    串接 Gemini API，根據 Top 關鍵詞生成 5 句洞察摘要和 120 字結論。\n",
        "    \"\"\"\n",
        "    if model is None or df_terms.empty:\n",
        "        return \"⚠️ Gemini API 未初始化或關鍵詞分析結果為空，無法生成 AI 洞察。\"\n",
        "\n",
        "    # 選擇前 10 個詞彙及其 TF-IDF 值\n",
        "    top_terms_data = df_terms.head(10)[[\"term\", \"tfidf_mean\", \"freq\", \"examples\"]].to_dict('records')\n",
        "\n",
        "    # 構建給 Gemini 的指令\n",
        "    system_prompt = (\n",
        "        \"你是一位專業的電影市場與輿情分析師。你的任務是根據輸入的 PTT 關鍵詞與統計數據，\"\n",
        "        \"總結出對當前電影話題的洞察。請務必嚴格遵循以下格式要求：\"\n",
        "        \"1. **洞察摘要**: 輸出 5 個精確的句子，每個句子都必須以編號 1. 到 5. 開頭。\"\n",
        "        \"2. **結論**: 輸出一段總長度約 120 字的總結段落。該結論必須在洞察摘要之後，並以標題 '## 分析結論' 開頭。\"\n",
        "        \"請只回傳請求的 Markdown 內容，不要包含任何額外的前言或後記。\"\n",
        "    )\n",
        "\n",
        "    user_query = f\"請根據以下 PTT 電影版熱門關鍵詞（已按 TF-IDF 平均值排序）進行分析：\\n\\n{json.dumps(top_terms_data, ensure_ascii=False)}\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            contents=[\n",
        "                {\"role\": \"user\", \"parts\": [{\"text\": user_query}]}\n",
        "            ],\n",
        "            system_instruction=system_prompt,\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Gemini API 呼叫失敗：{e}\"\n",
        "\n",
        "\n",
        "def full_analysis_workflow(index_pages, min_push, keyword, topk, min_df):\n",
        "    \"\"\"\n",
        "    一鍵執行完整的爬蟲、分析和 AI 洞察流程。\n",
        "    \"\"\"\n",
        "    # 1. 爬蟲階段\n",
        "    msg_crawl, df_posts = crawl_ptt_movie(index_pages, min_push, keyword)\n",
        "\n",
        "    # 2. 文本分析階段 (從 Sheet 讀回最新數據)\n",
        "    global ptt_posts_df\n",
        "    ptt_posts_df = df_posts # 使用剛爬取/合併的最新 DF 進行分析\n",
        "    msg_analysis, df_terms, md_analysis = analyze_ptt_texts(topk=topk, min_df=min_df)\n",
        "\n",
        "    # 3. AI 洞察生成階段\n",
        "    gemini_output = generate_insights(df_terms, GEMINI_MODEL)\n",
        "\n",
        "    # 4. 組合所有結果\n",
        "    final_report = f\"\"\"\n",
        "# PTT 電影版輿情分析報告\n",
        "\n",
        "## 流程狀態\n",
        "- 爬蟲結果：{msg_crawl}\n",
        "- 文本分析：{msg_analysis}\n",
        "\n",
        "---\n",
        "\n",
        "## 關鍵詞與詞頻統計\n",
        "{md_analysis}\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Gemini AI 洞察與總結\n",
        "\n",
        "{gemini_output}\n",
        "\"\"\"\n",
        "    return final_report, df_terms\n",
        "\n",
        "def _refresh_data():\n",
        "    \"\"\"重新從 Sheet 讀取最新數據到內存\"\"\"\n",
        "    global ptt_posts_df, terms_df\n",
        "    ptt_posts_df = read_ptt_posts_df()\n",
        "    terms_df = read_terms_df()\n",
        "    return ptt_posts_df, terms_df\n"
      ],
      "metadata": {
        "id": "c2-aQ7vzClKh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入最新的數據以供初始化顯示\n",
        "_refresh_data()\n",
        "\n",
        "with gr.Blocks(title=\"PTT 電影版 AI 輿情分析\") as demo:\n",
        "    gr.Markdown(\"# 🎬 PTT 電影版 AI 輿情分析自動化工具\")\n",
        "    gr.Markdown(\"此工具會執行：**PTT 爬蟲 → 寫入 Sheet → TF-IDF 關鍵詞分析 → 寫入統計表 → Gemini AI 洞察生成**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(f\"**Google Sheet URL:** [分析試算表連結]({SHEET_URL})\")\n",
        "\n",
        "    with gr.Tab(\"一鍵分析執行區\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 爬蟲參數 (Crawl Settings)\")\n",
        "                index_pages = gr.Number(label=\"往前翻頁數 (Pages)\", value=3, precision=0)\n",
        "                min_push = gr.Number(label=\"最小推文數 (Min Nrec)\", value=10, precision=0)\n",
        "                keyword = gr.Textbox(label=\"標題關鍵詞篩選 (Keyword)\", value=\"\", placeholder=\"可選，例如：阿凡達\")\n",
        "\n",
        "                gr.Markdown(\"### 文本分析參數 (Analysis Settings)\")\n",
        "                topk = gr.Number(label=\"輸出 Top K 關鍵詞\", value=50, precision=0)\n",
        "                min_df = gr.Number(label=\"最小文件頻率 (Min DF)\", value=2, precision=0, info=\"詞彙至少要在幾篇文章中出現\")\n",
        "\n",
        "                btn_run = gr.Button(\"🚀 一鍵執行完整流程並生成報告\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### 🤖 分析報告 (Analysis Report)\")\n",
        "                report_output = gr.Markdown(\"點擊按鈕開始分析...\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 💾 關鍵詞統計結果預覽 (Top Terms Preview)\")\n",
        "        grid_terms = gr.Dataframe(value=terms_df, label=\"關鍵詞統計數據 (已寫入 Sheet)\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"原始貼文預覽\"):\n",
        "        gr.Markdown(\"### 原始貼文數據 (已累積在 Sheet 中)\")\n",
        "        grid_posts = gr.Dataframe(value=ptt_posts_df, label=\"PTT 文章資料\", interactive=False)\n",
        "        btn_refresh_data = gr.Button(\"⬇️ 從 Sheet 讀取最新數據\")\n",
        "\n",
        "    # --- 綁定動作 ---\n",
        "\n",
        "    # 執行流程\n",
        "    btn_run.click(\n",
        "        full_analysis_workflow,\n",
        "        inputs=[index_pages, min_push, keyword, topk, min_df],\n",
        "        outputs=[report_output, grid_terms]\n",
        "    )\n",
        "\n",
        "    # 重新讀取數據\n",
        "    btn_refresh_data.click(\n",
        "        _refresh_data,\n",
        "        outputs=[grid_posts, grid_terms]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "b2hVyzl7ClMn",
        "outputId": "d4c68d48-75c2-4a77-83e4-10dcbd40df10"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://247a988d55b4f26592.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://247a988d55b4f26592.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}